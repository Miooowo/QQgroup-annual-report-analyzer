# -*- coding: utf-8 -*-
"""
åˆ†è¯å™¨å°è£…ï¼šæ”¯æŒjiebaã€pkusegå’Œsubwordç®—æ³•ï¼ˆSentencePieceï¼‰
ä¼˜åŒ–ä¸­æ–‡åˆ†è¯æ•ˆæœ
"""

import re
import os
from typing import List, Optional
import jieba

# å°è¯•å¯¼å…¥sentencepiece
try:
    import sentencepiece as spm
    SENTENCEPIECE_AVAILABLE = True
except ImportError:
    SENTENCEPIECE_AVAILABLE = False
    spm = None

# å°è¯•å¯¼å…¥pkuseg
try:
    import pkuseg
    PKUSEG_AVAILABLE = True
except ImportError:
    PKUSEG_AVAILABLE = False
    pkuseg = None


class TokenizerWrapper:
    """åˆ†è¯å™¨å°è£…ç±»ï¼Œæ”¯æŒå¤šç§åˆ†è¯ç®—æ³•"""
    
    def __init__(self, tokenizer_type='jieba', model_path=None, 
                 use_hmm=True, use_paddle=False, custom_dict_files=None):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨
        
        Args:
            tokenizer_type: åˆ†è¯å™¨ç±»å‹ï¼Œå¯é€‰ 'jieba'ã€'pkuseg' æˆ– 'subword' (SentencePiece)
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆpkusegé¢†åŸŸæ¨¡å‹æˆ–SentencePieceæ¨¡å‹ï¼‰
            use_hmm: æ˜¯å¦ä½¿ç”¨HMMæ¨¡å‹ï¼ˆä»…jiebaï¼‰
            use_paddle: æ˜¯å¦ä½¿ç”¨paddleæ¨¡å¼ï¼ˆä»…jiebaï¼Œéœ€è¦å®‰è£…paddlepaddleï¼‰
            custom_dict_files: è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶åˆ—è¡¨
        """
        self.tokenizer_type = tokenizer_type
        self.model_path = model_path
        self.use_hmm = use_hmm
        self.use_paddle = use_paddle
        self.sp_model = None
        self.pkuseg_model = None
        self.custom_words = set()  # è‡ªå®šä¹‰è¯æ±‡é›†åˆ
        
        # å¤„ç†subwordæ¨¡å¼
        if tokenizer_type == 'subword':
            if not SENTENCEPIECE_AVAILABLE:
                print("âš ï¸ SentencePieceæœªå®‰è£…ï¼Œå›é€€åˆ°jiebaåˆ†è¯")
                print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install sentencepiece")
                self.tokenizer_type = 'jieba'
            elif model_path and os.path.exists(model_path):
                try:
                    self.sp_model = spm.SentencePieceProcessor()
                    self.sp_model.load(model_path)
                    print(f"âœ… åŠ è½½SentencePieceæ¨¡å‹: {model_path}")
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½SentencePieceæ¨¡å‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°jiebaåˆ†è¯")
                    self.tokenizer_type = 'jieba'
            else:
                print("âš ï¸ æœªæä¾›SentencePieceæ¨¡å‹ï¼Œå›é€€åˆ°jiebaåˆ†è¯")
                self.tokenizer_type = 'jieba'
        
        # å¤„ç†pkusegæ¨¡å¼
        elif tokenizer_type == 'pkuseg':
            if not PKUSEG_AVAILABLE:
                print("âš ï¸ pkusegæœªå®‰è£…ï¼Œå›é€€åˆ°jiebaåˆ†è¯")
                print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install pkuseg")
                self.tokenizer_type = 'jieba'
            else:
                try:
                    # pkusegæ”¯æŒé¢†åŸŸæ¨¡å‹ï¼šnews, web, medicine, tourismç­‰
                    if model_path and model_path in ['news', 'web', 'medicine', 'tourism']:
                        self.pkuseg_model = pkuseg.pkuseg(model_name=model_path)
                        print(f"âœ… åŠ è½½pkusegé¢†åŸŸæ¨¡å‹: {model_path}")
                    else:
                        # ä½¿ç”¨é»˜è®¤æ¨¡å‹
                        self.pkuseg_model = pkuseg.pkuseg()
                        print("âœ… ä½¿ç”¨pkusegé»˜è®¤æ¨¡å‹")
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½pkusegæ¨¡å‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°jiebaåˆ†è¯")
                    self.tokenizer_type = 'jieba'
        
        # åˆå§‹åŒ–jieba
        if self.tokenizer_type == 'jieba':
            jieba.setLogLevel(jieba.logging.INFO)
            # åŠ è½½è‡ªå®šä¹‰è¯å…¸
            self._load_custom_dicts(custom_dict_files)
            print("âœ… ä½¿ç”¨jiebaåˆ†è¯å™¨")
    
    def _load_custom_dicts(self, custom_dict_files):
        """åŠ è½½è‡ªå®šä¹‰è¯å…¸æ–‡ä»¶"""
        if not custom_dict_files:
            return
        
        loaded_count = 0
        for dict_file in custom_dict_files:
            # æ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
            if not os.path.isabs(dict_file):
                # ç›¸å¯¹è·¯å¾„ï¼šå…ˆå°è¯•å½“å‰ç›®å½•ï¼Œå†å°è¯•é¡¹ç›®æ ¹ç›®å½•
                possible_paths = [
                    dict_file,
                    os.path.join(os.path.dirname(__file__), dict_file),
                    os.path.join(os.path.dirname(os.path.dirname(__file__)), dict_file)
                ]
            else:
                possible_paths = [dict_file]
            
            loaded = False
            for path in possible_paths:
                if os.path.exists(path):
                    try:
                        jieba.load_userdict(path)
                        print(f"âœ… åŠ è½½è‡ªå®šä¹‰è¯å…¸: {path}")
                        loaded_count += 1
                        loaded = True
                        break
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½è¯å…¸æ–‡ä»¶å¤±è´¥ {path}: {e}")
                        break
            
            if not loaded:
                print(f"âš ï¸ æœªæ‰¾åˆ°è¯å…¸æ–‡ä»¶: {dict_file}")
        
        if loaded_count > 0:
            print(f"ğŸ“š å…±åŠ è½½ {loaded_count} ä¸ªè‡ªå®šä¹‰è¯å…¸æ–‡ä»¶")
    
    def add_word(self, word: str, freq: int = 1000):
        """
        æ·»åŠ è‡ªå®šä¹‰è¯æ±‡
        
        Args:
            word: è¯æ±‡
            freq: è¯é¢‘
        """
        if self.tokenizer_type == 'jieba':
            jieba.add_word(word, freq=freq)
        elif self.tokenizer_type == 'pkuseg':
            # pkusegä¸æ”¯æŒåŠ¨æ€æ·»åŠ è¯æ±‡ï¼Œè®°å½•ç”¨äºåå¤„ç†
            self.custom_words.add(word)
        else:
            # subwordæ¨¡å¼ä¸‹ï¼Œè®°å½•è‡ªå®šä¹‰è¯æ±‡ç”¨äºåå¤„ç†
            self.custom_words.add(word)
    
    def cut(self, text: str, cut_all: bool = False) -> List[str]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            cut_all: æ˜¯å¦å…¨æ¨¡å¼åˆ†è¯ï¼ˆä»…jiebaï¼‰
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if not text or not text.strip():
            return []
        
        if self.tokenizer_type == 'jieba':
            # jiebaä¼˜åŒ–é…ç½®
            if cut_all:
                # å…¨æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰å¯èƒ½çš„åˆ†è¯ç»“æœ
                words = list(jieba.cut(text, cut_all=True, HMM=False))
            else:
                # ç²¾ç¡®æ¨¡å¼ï¼šä½¿ç”¨HMMè¯†åˆ«æœªç™»å½•è¯
                words = list(jieba.cut(text, cut_all=False, HMM=self.use_hmm))
            
            # è¿‡æ»¤ç©ºç™½å­—ç¬¦
            words = [w.strip() for w in words if w.strip()]
            return words
            
        elif self.tokenizer_type == 'pkuseg':
            # pkusegåˆ†è¯
            words = self.pkuseg_model.cut(text)
            # è¿‡æ»¤ç©ºç™½å­—ç¬¦
            words = [w.strip() for w in words if w.strip()]
            return words
            
        else:
            # ä½¿ç”¨SentencePieceè¿›è¡Œsubwordåˆ†è¯
            return self._subword_tokenize(text)
    
    def _subword_tokenize(self, text: str) -> List[str]:
        """
        ä½¿ç”¨SentencePieceè¿›è¡Œsubwordåˆ†è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åˆ†è¯ç»“æœåˆ—è¡¨
        """
        if not self.sp_model:
            # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œå›é€€åˆ°å­—ç¬¦çº§åˆ†è¯
            return list(text)
        
        try:
            # SentencePieceåˆ†è¯
            pieces = self.sp_model.encode(text, out_type=str)
            
            # åå¤„ç†ï¼šåˆå¹¶è‡ªå®šä¹‰è¯æ±‡
            if self.custom_words:
                pieces = self._merge_custom_words(pieces, text)
            
            # è¿‡æ»¤ç©ºç™½å­—ç¬¦
            pieces = [p.strip() for p in pieces if p.strip()]
            return pieces
        except Exception as e:
            print(f"âš ï¸ SentencePieceåˆ†è¯å¤±è´¥: {e}ï¼Œä½¿ç”¨å­—ç¬¦çº§åˆ†è¯")
            return [c for c in text if c.strip()]
    
    def _merge_custom_words(self, pieces: List[str], original_text: str) -> List[str]:
        """
        åˆå¹¶è‡ªå®šä¹‰è¯æ±‡ï¼ˆå°†subwordç‰‡æ®µåˆå¹¶ä¸ºå®Œæ•´è¯ï¼‰
        
        Args:
            pieces: subwordç‰‡æ®µåˆ—è¡¨
            original_text: åŸå§‹æ–‡æœ¬
            
        Returns:
            åˆå¹¶åçš„è¯æ±‡åˆ—è¡¨
        """
        if not self.custom_words:
            return pieces
        
        result = []
        i = 0
        text_lower = original_text.lower()
        
        while i < len(pieces):
            matched = False
            # æ£€æŸ¥æ˜¯å¦èƒ½åŒ¹é…è‡ªå®šä¹‰è¯æ±‡
            for custom_word in sorted(self.custom_words, key=len, reverse=True):
                custom_lower = custom_word.lower()
                # æ£€æŸ¥ä»å½“å‰ä½ç½®å¼€å§‹çš„ç‰‡æ®µæ˜¯å¦èƒ½ç»„æˆè‡ªå®šä¹‰è¯
                remaining_pieces = ''.join(pieces[i:])
                if remaining_pieces.startswith(custom_lower) or custom_lower in remaining_pieces:
                    # å°è¯•åŒ¹é…
                    matched_pieces = []
                    matched_text = ''
                    j = i
                    while j < len(pieces) and len(matched_text) < len(custom_word):
                        matched_pieces.append(pieces[j])
                        matched_text = ''.join(matched_pieces)
                        if matched_text == custom_word or matched_text == custom_lower:
                            result.append(custom_word)
                            i = j + 1
                            matched = True
                            break
                        j += 1
                    if matched:
                        break
            
            if not matched:
                result.append(pieces[i])
                i += 1
        
        return result
    
    def train_model(self, texts: List[str], output_model_path: str, 
                   vocab_size: int = 8000, model_type: str = 'bpe'):
        """
        è®­ç»ƒSentencePieceæ¨¡å‹
        
        Args:
            texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
            output_model_path: è¾“å‡ºæ¨¡å‹è·¯å¾„
            vocab_size: è¯æ±‡è¡¨å¤§å°
            model_type: æ¨¡å‹ç±»å‹ï¼Œ'bpe' æˆ– 'unigram'
        """
        if not SENTENCEPIECE_AVAILABLE:
            print("âŒ SentencePieceæœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            return False
        
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            temp_file = output_model_path + '.train.txt'
            with open(temp_file, 'w', encoding='utf-8') as f:
                for text in texts:
                    f.write(text + '\n')
            
            # è®­ç»ƒå‚æ•°
            spm.SentencePieceTrainer.train(
                input=temp_file,
                model_prefix=output_model_path.replace('.model', ''),
                vocab_size=vocab_size,
                model_type=model_type,
                character_coverage=0.9995,  # å­—ç¬¦è¦†ç›–ç‡
                max_sentence_length=4192,
                shuffle_input_sentence=True,
                input_sentence_size=1000000,  # é™åˆ¶è®­ç»ƒæ•°æ®é‡
                seed_sentencepiece_size=1000000,
                shrinking_factor=0.75,
                num_threads=4,
                num_sub_iterations=2
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)
            
            print(f"âœ… SentencePieceæ¨¡å‹è®­ç»ƒå®Œæˆ: {output_model_path}.model")
            return True
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒSentencePieceæ¨¡å‹å¤±è´¥: {e}")
            return False


def create_subword_tokenizer_from_data(texts: List[str], vocab_size: int = 8000, 
                                      model_dir: str = 'models') -> Optional[TokenizerWrapper]:
    """
    ä»æ•°æ®è®­ç»ƒå¹¶åˆ›å»ºsubwordåˆ†è¯å™¨
    
    Args:
        texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
        vocab_size: è¯æ±‡è¡¨å¤§å°
        model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        
    Returns:
        TokenizerWrapperå®ä¾‹ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    if not SENTENCEPIECE_AVAILABLE:
        print("âš ï¸ SentencePieceæœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºsubwordåˆ†è¯å™¨")
        return None
    
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, 'spm_model.model')
    
    # å¦‚æœæ¨¡å‹å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    if os.path.exists(model_path):
        return TokenizerWrapper(tokenizer_type='subword', model_path=model_path)
    
    # è®­ç»ƒæ–°æ¨¡å‹
    wrapper = TokenizerWrapper(tokenizer_type='subword')
    if wrapper.train_model(texts, model_path, vocab_size=vocab_size):
        wrapper.sp_model = spm.SentencePieceProcessor()
        wrapper.sp_model.load(model_path)
        wrapper.model_path = model_path
        return wrapper
    
    return None
