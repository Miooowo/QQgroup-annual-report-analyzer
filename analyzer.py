# -*- coding: utf-8 -*-
import re
import random
import string
import math
import jieba
from collections import Counter, defaultdict
import config as cfg
from utils import (
    extract_emojis,
    is_emoji,
    parse_timestamp,
    clean_text,
    calculate_entropy,
    analyze_single_chars,
    analyze_sentiment,
    MEANINGLESS_SYMBOLS,
)
from tokenizer_wrapper import TokenizerWrapper

jieba.setLogLevel(jieba.logging.INFO)

class ChatAnalyzer:
    def __init__(self, data):
        self.data = data
        self.messages = data.get('messages', [])
        self.chat_name = data.get('chatName', data.get('chatInfo', {}).get('name', 'æœªçŸ¥ç¾¤èŠ'))
        self.uin_to_name = {}
        self.msgid_to_sender = {}
        self.word_freq = Counter()
        self.word_samples = defaultdict(list)
        self.word_contributors = defaultdict(Counter)
        self.user_msg_count = Counter()
        self.user_char_count = Counter()
        self.user_char_per_msg = {}
        self.user_image_count = Counter()
        self.user_forward_count = Counter()
        self.user_reply_count = Counter()
        self.user_replied_count = Counter()
        self.user_at_count = Counter()
        self.user_ated_count = Counter()
        self.user_emoji_count = Counter()
        self.user_link_count = Counter()
        self.user_night_count = Counter()
        self.user_morning_count = Counter()
        self.user_repeat_count = Counter()
        self.hour_distribution = Counter()
        self.discovered_words = set()
        self.merged_words = {}
        self.single_char_stats = {}  # å•å­—ç»Ÿè®¡
        self.cleaned_texts = []  # ç¼“å­˜æ¸…æ´—åçš„æ–‡æœ¬
        # æ–°å¢ï¼šç”¨æˆ·æƒ…æ„Ÿç»Ÿè®¡
        self.user_positive_count = Counter()  # æ­£å‘æƒ…æ„Ÿå‘è¨€æ•°
        self.user_negative_count = Counter()  # è´Ÿå‘æƒ…æ„Ÿå‘è¨€æ•°
        self.user_neutral_count = Counter()  # ä¸­ç«‹æƒ…æ„Ÿå‘è¨€æ•°
        # æ–°å¢ï¼šç”¨æˆ·@ä»–äººç»Ÿè®¡
        self.user_at_targets = defaultdict(Counter)  # {uin: {target_uin: count}}
        # æ–°å¢ï¼šç”¨æˆ·å‘è¨€æ ·æœ¬ï¼ˆç”¨äºAIä¸¾ä¾‹ï¼‰
        self.user_message_samples = defaultdict(list)  # {uin: [message_texts]}
        # åŒè¯å¼‚æ ¼æ˜ å°„ï¼ˆåˆ«ååˆ°æ ‡å‡†è¯çš„æ˜ å°„ï¼‰
        self.word_alias_map = getattr(cfg, 'WORD_ALIAS_MAP', {})
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer_type = getattr(cfg, 'TOKENIZER_TYPE', 'jieba')
        model_path = getattr(cfg, 'SP_MODEL_PATH', None) or getattr(cfg, 'PKUSEG_MODEL', None)
        use_hmm = getattr(cfg, 'JIEBA_USE_HMM', True)
        use_paddle = getattr(cfg, 'JIEBA_USE_PADDLE', False)
        custom_dict_files = getattr(cfg, 'CUSTOM_DICT_FILES', [])
        self.tokenizer = TokenizerWrapper(
            tokenizer_type=tokenizer_type, 
            model_path=model_path,
            use_hmm=use_hmm,
            use_paddle=use_paddle,
            custom_dict_files=custom_dict_files
        )
        self._build_mappings()
        # æ ¹æ®ç¾¤èŠåç§°æ·»åŠ ç‰¹å®šè¯æ±‡
        self._add_chat_name_words()

    def _is_bot_message(self, msg):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœºå™¨äººæ¶ˆæ¯ï¼ˆåŸºäº subMsgTypeï¼‰"""
        if not cfg.FILTER_BOT_MESSAGES:
            return False
        
        raw_msg = msg.get('rawMessage', {})
        sub_msg_type = raw_msg.get('subMsgType', 0)
        return sub_msg_type in [577, 65]
    
    def _should_filter_user(self, msg):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿‡æ»¤è¯¥ç”¨æˆ·çš„æ¶ˆæ¯"""
        sender = msg.get('sender', {})
        name = sender.get('name', '').strip()
        uin = sender.get('uin', '')
        
        # æ£€æŸ¥ç”¨æˆ·åæ˜¯å¦åœ¨è¿‡æ»¤åˆ—è¡¨ä¸­
        if name:
            for filtered_name in cfg.FILTERED_USERS:
                if filtered_name in name:
                    return True
        
        # æ£€æŸ¥ sendMemberName
        raw_msg = msg.get('rawMessage', {})
        send_member_name = raw_msg.get('sendMemberName', '').strip()
        if send_member_name:
            for filtered_name in cfg.FILTERED_USERS:
                if filtered_name in send_member_name:
                    return True
        
        # æ£€æŸ¥ uin_to_name æ˜ å°„ä¸­çš„åç§°
        if uin and uin in self.uin_to_name:
            mapped_name = self.uin_to_name[uin]
            for filtered_name in cfg.FILTERED_USERS:
                if filtered_name in mapped_name:
                    return True
        
        return False

    def _build_mappings(self):
        """æ„å»º uin åˆ° name çš„æ˜ å°„ï¼Œä¼˜å…ˆä¿ç•™æœ‰æ•ˆçš„ name"""
        # å…ˆæ”¶é›†æ¯ä¸ª uin çš„æ‰€æœ‰ nameï¼ˆæŒ‰é¡ºåºï¼‰å’Œ sendMemberName
        uin_names = defaultdict(list)
        uin_member_names = {}  # å­˜å‚¨æœ€åçš„ sendMemberName
        
        for msg in self.messages:
            # è·³è¿‡æœºå™¨äººæ¶ˆæ¯
            if self._is_bot_message(msg):
                continue
            
            # è·³è¿‡è¢«è¿‡æ»¤çš„ç”¨æˆ·ï¼ˆåœ¨æ„å»ºæ˜ å°„æ—¶ä¹Ÿè¦è¿‡æ»¤ï¼Œé¿å…å°†è¿‡æ»¤ç”¨æˆ·åŠ å…¥æ˜ å°„ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å…ˆæ£€æŸ¥ sender.nameï¼Œå› ä¸º uin_to_name æ˜ å°„è¿˜æœªæ„å»ºå®Œæˆ
            sender = msg.get('sender', {})
            name = sender.get('name', '').strip()
            raw_msg = msg.get('rawMessage', {})
            send_member_name = raw_msg.get('sendMemberName', '').strip()
            
            # ç®€å•æ£€æŸ¥ç”¨æˆ·åæ˜¯å¦åŒ…å«è¿‡æ»¤å…³é”®è¯
            should_filter = False
            if name:
                for filtered_name in cfg.FILTERED_USERS:
                    if filtered_name in name:
                        should_filter = True
                        break
            if not should_filter and send_member_name:
                for filtered_name in cfg.FILTERED_USERS:
                    if filtered_name in send_member_name:
                        should_filter = True
                        break
            
            if should_filter:
                continue
            
            uin = sender.get('uin')
            msg_id = msg.get('messageId')
            
            # æ”¶é›† name
            if uin and name:
                # åªåœ¨ name ä¸ä¸Šä¸€ä¸ªä¸åŒæ—¶æ·»åŠ 
                if not uin_names[uin] or uin_names[uin][-1] != name:
                    uin_names[uin].append(name)
            
            # æ”¶é›† sendMemberNameï¼ˆä¿ç•™æœ€åä¸€ä¸ªï¼‰
            if uin:
                raw_msg = msg.get('rawMessage', {})
                send_member_name = raw_msg.get('sendMemberName', '').strip()
                if send_member_name:
                    uin_member_names[uin] = send_member_name
            
            if msg_id and uin:
                self.msgid_to_sender[msg_id] = uin
        
        # ä¸ºæ¯ä¸ª uin é€‰æ‹©æœ€åˆé€‚çš„ name
        for uin, names in uin_names.items():
            # ä»åå¾€å‰æ‰¾ç¬¬ä¸€ä¸ªä¸ç­‰äºuinçš„ name
            chosen_name = None
            for name in reversed(names):
                if name != str(uin):
                    chosen_name = name
                    break
            
            # å¦‚æœæ‰€æœ‰ name éƒ½ç­‰äº uinï¼Œä½¿ç”¨ sendMemberName
            if chosen_name is None:
                if uin in uin_member_names:
                    chosen_name = uin_member_names[uin]
                elif names:
                    chosen_name = names[-1]  # å…œåº•ï¼šä½¿ç”¨æœ€åä¸€ä¸ª
            
            if chosen_name:
                self.uin_to_name[uin] = chosen_name

    def get_name(self, uin):
        return self.uin_to_name.get(uin, f"æœªçŸ¥ç”¨æˆ·({uin})")
    
    def _add_chat_name_words(self):
        """æ ¹æ®ç¾¤èŠåç§°æ·»åŠ ç‰¹å®šè¯æ±‡åˆ°è¯å…¸"""
        chat_name_words = getattr(cfg, 'CHAT_NAME_WORDS', {})
        if not chat_name_words:
            return
        
        # æ£€æŸ¥ç¾¤èŠåç§°æ˜¯å¦åŒ¹é…
        added_words = []
        for chat_keyword, words_to_add in chat_name_words.items():
            if chat_keyword in self.chat_name:
                for word in words_to_add:
                    # æ·»åŠ åˆ°åˆ†è¯å™¨è¯å…¸
                    self.tokenizer.add_word(word, freq=2000)  # è®¾ç½®è¾ƒé«˜è¯é¢‘ï¼Œç¡®ä¿è¢«è¯†åˆ«
                    added_words.append(word)
                    print(f"   ğŸ“ æ ¹æ®ç¾¤åã€Œ{self.chat_name}ã€æ·»åŠ è¯æ±‡: {word}")
        
        if added_words:
            print(f"   âœ… å…±æ·»åŠ  {len(added_words)} ä¸ªç¾¤åç›¸å…³è¯æ±‡: {', '.join(added_words)}")

    def _normalize_word(self, word):
        """åŒè¯å¼‚æ ¼å¤„ç†ï¼šå°†åˆ«åæ˜ å°„åˆ°æ ‡å‡†è¯"""
        if word in self.word_alias_map:
            return self.word_alias_map[word]
        return word
    
    def _is_id_like_string(self, word):
        """åˆ¤æ–­æ˜¯å¦ä¸ºIDç±»å­—ç¬¦ä¸²ï¼ˆå›¾ç‰‡IDã€æ¶ˆæ¯IDç­‰ï¼‰"""
        if not word:
            return False
        
        # é•¿åº¦æ£€æŸ¥ï¼šIDé€šå¸¸åœ¨3-20ä¸ªå­—ç¬¦ä¹‹é—´ï¼ˆåŒ…æ‹¬çŸ­IDå¦‚7R%D8ã€0ED3Vï¼‰
        if len(word) < 3 or len(word) > 20:
            return False
        
        # å¦‚æœåŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚%ã€_ã€-ã€}ã€]ç­‰ï¼‰ï¼Œå¾ˆå¯èƒ½æ˜¯ID
        if re.search(r'[%_\-}\]]', word):
            return True
        
        # å¿…é¡»æ˜¯å­—æ¯æ•°å­—ç»„åˆï¼ˆä¸åŒ…å«ä¸­æ–‡ã€æ ‡ç‚¹ç­‰ï¼‰
        if not re.match(r'^[a-zA-Z0-9]+$', word):
            return False
        
        # å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå­—æ¯å’Œä¸€ä¸ªæ•°å­—
        has_letter = bool(re.search(r'[a-zA-Z]', word))
        has_digit = bool(re.search(r'[0-9]', word))
        
        if not (has_letter and has_digit):
            return False
        
        # å¯¹äºçŸ­å­—ç¬¦ä¸²ï¼ˆ3-5ä¸ªå­—ç¬¦ï¼‰ï¼Œå¦‚æœå­—æ¯æ•°å­—æ··åˆï¼Œå¾ˆå¯èƒ½æ˜¯ID
        if len(word) <= 5:
            # å¦‚æœå…¨æ˜¯æ•°å­—æˆ–å…¨æ˜¯å­—æ¯ï¼Œä¸æ˜¯ID
            if re.match(r'^[0-9]+$', word) or re.match(r'^[a-zA-Z]+$', word):
                return False
            # å­—æ¯æ•°å­—æ··åˆçš„çŸ­å­—ç¬¦ä¸²ï¼Œå¾ˆå¯èƒ½æ˜¯ID
            return True
        
        # å¯¹äºé•¿å­—ç¬¦ä¸²ï¼ˆ6-20ä¸ªå­—ç¬¦ï¼‰ï¼Œå­—æ¯æ•°é‡åº”è¯¥å å¤šæ•°ï¼ˆè‡³å°‘50%ï¼‰
        letter_count = len(re.findall(r'[a-zA-Z]', word))
        if letter_count < len(word) * 0.5:
            return False
        
        # æ’é™¤å¸¸è§çš„è‹±æ–‡å•è¯ï¼ˆé•¿åº¦åœ¨6-20ä¹‹é—´çš„å¸¸è§è¯ï¼‰
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šå¸¸è§è¯ï¼Œä½†ä¸ºäº†æ€§èƒ½ï¼Œåªæ£€æŸ¥ä¸€äº›æ˜æ˜¾çš„
        common_words = {'password', 'username', 'account', 'message', 'picture', 'image'}
        if word.lower() in common_words:
            return False
        
        return True

    def analyze(self):
        print(f"ğŸ“Š å¼€å§‹åˆ†æ: {self.chat_name}")
        print(f"ğŸ“ æ¶ˆæ¯æ•°: {len(self.messages)}")
        print("=" * cfg.CONSOLE_WIDTH)
        
        print("\nğŸ§¹ é¢„å¤„ç†æ–‡æœ¬...")
        self._preprocess_texts()
        
        # å¦‚æœä½¿ç”¨subwordåˆ†è¯å™¨ä¸”æ²¡æœ‰æ¨¡å‹ï¼Œå°è¯•ä»æ•°æ®è®­ç»ƒ
        if (self.tokenizer.tokenizer_type == 'subword' and 
            not self.tokenizer.sp_model and 
            len(self.cleaned_texts) > 100):
            print("ğŸ”§ è®­ç»ƒSentencePieceæ¨¡å‹...")
            from tokenizer_wrapper import create_subword_tokenizer_from_data
            vocab_size = getattr(cfg, 'SP_VOCAB_SIZE', 8000)
            # ä½¿ç”¨éƒ¨åˆ†æ•°æ®è®­ç»ƒï¼ˆé¿å…å¤ªæ…¢ï¼‰
            train_texts = self.cleaned_texts[:min(10000, len(self.cleaned_texts))]
            new_tokenizer = create_subword_tokenizer_from_data(
                train_texts, 
                vocab_size=vocab_size
            )
            if new_tokenizer:
                self.tokenizer = new_tokenizer
        
        print("ğŸ”¤ åˆ†æå•å­—ç‹¬ç«‹æ€§...")
        self.single_char_stats = analyze_single_chars(self.cleaned_texts)
        
        print("ğŸ” æ–°è¯å‘ç°...")
        self._discover_new_words()
        
        print("ğŸ”— è¯ç»„åˆå¹¶...")
        self._merge_word_pairs()
        
        print("ğŸ“ˆ åˆ†è¯ç»Ÿè®¡...")
        self._tokenize_and_count()
        
        print("ğŸ® è¶£å‘³ç»Ÿè®¡...")
        self._fun_statistics()
        
        print("ğŸ§¹ è¿‡æ»¤æ•´ç†...")
        self._filter_results()
        
        print("\nâœ… å®Œæˆ!")

    def _preprocess_texts(self):
        """é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬"""
        skipped = 0
        bot_filtered = 0
        for msg in self.messages:
            # è·³è¿‡æœºå™¨äººæ¶ˆæ¯
            if self._is_bot_message(msg):
                bot_filtered += 1
                continue
            
            # è·³è¿‡è¢«è¿‡æ»¤çš„ç”¨æˆ·
            if self._should_filter_user(msg):
                bot_filtered += 1
                continue
            
            content = msg.get('content', {})
            text = content.get('text', '') if isinstance(content, dict) else ''
            cleaned = clean_text(text)
            if cleaned and len(cleaned) >= 1:
                self.cleaned_texts.append(cleaned)
            elif text:
                skipped += 1
        
        if cfg.FILTER_BOT_MESSAGES and bot_filtered > 0:
            print(f"   æœ‰æ•ˆæ–‡æœ¬: {len(self.cleaned_texts)} æ¡, è·³è¿‡: {skipped} æ¡, è¿‡æ»¤æœºå™¨äºº: {bot_filtered} æ¡")
        else:
            print(f"   æœ‰æ•ˆæ–‡æœ¬: {len(self.cleaned_texts)} æ¡, è·³è¿‡: {skipped} æ¡")

    def _discover_new_words(self):
        """æ–°è¯å‘ç°"""
        ngram_freq = Counter()
        left_neighbors = defaultdict(Counter)
        right_neighbors = defaultdict(Counter)
        total_chars = 0
        
        for text in self.cleaned_texts:
            # æŒ‰æ ‡ç‚¹åˆ†å¥
            sentences = re.split(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰\s\n\r,\.!?\(\)]', text)
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 2:
                    continue
                total_chars += len(sentence)
                
                for n in range(2, min(6, len(sentence) + 1)):
                    for i in range(len(sentence) - n + 1):
                        ngram = sentence[i:i+n]
                        # è·³è¿‡çº¯æ•°å­—/ç¬¦å·/çº¯è‹±æ–‡
                        if re.match(r'^[\d\s\W]+$', ngram) or re.match(r'^[a-zA-Z]+$', ngram):
                            continue
                        ngram_freq[ngram] += 1
                        if i > 0:
                            left_neighbors[ngram][sentence[i-1]] += 1
                        else:
                            left_neighbors[ngram]['<BOS>'] += 1
                        if i + n < len(sentence):
                            right_neighbors[ngram][sentence[i+n]] += 1
                        else:
                            right_neighbors[ngram]['<EOS>'] += 1
        
        # ç­›é€‰æ–°è¯
        for word, freq in ngram_freq.items():
            if freq < cfg.NEW_WORD_MIN_FREQ:
                continue
            
            # é‚»æ¥ç†µ
            left_ent = calculate_entropy(left_neighbors[word])
            right_ent = calculate_entropy(right_neighbors[word])
            min_ent = min(left_ent, right_ent)
            if min_ent < cfg.ENTROPY_THRESHOLD:
                continue
            
            # PMIï¼ˆå†…éƒ¨å‡èšåº¦ï¼‰
            min_pmi = float('inf')
            for i in range(1, len(word)):
                left_freq = ngram_freq.get(word[:i], 0)
                right_freq = ngram_freq.get(word[i:], 0)
                if left_freq > 0 and right_freq > 0:
                    pmi = math.log2((freq * total_chars) / (left_freq * right_freq + 1e-10))
                    min_pmi = min(min_pmi, pmi)
            
            if min_pmi == float('inf'):
                min_pmi = 0
            
            if min_pmi < cfg.PMI_THRESHOLD:
                continue
            
            self.discovered_words.add(word)
        
        # æ·»åŠ åˆ°åˆ†è¯å™¨è¯å…¸
        for word in self.discovered_words:
            self.tokenizer.add_word(word, freq=1000)
        
        print(f"   å‘ç° {len(self.discovered_words)} ä¸ªæ–°è¯")

    def _merge_word_pairs(self):
        """è¯ç»„åˆå¹¶"""
        bigram_counter = Counter()
        word_right_counter = Counter()
        
        for text in self.cleaned_texts:
            words = [w for w in self.tokenizer.cut(text) if w.strip()]
            for i in range(len(words) - 1):
                w1, w2 = words[i].strip(), words[i+1].strip()
                if not w1 or not w2:
                    continue
                if re.match(r'^[\d\W]+$', w1) or re.match(r'^[\d\W]+$', w2):
                    continue
                bigram_counter[(w1, w2)] += 1
                word_right_counter[w1] += 1
        
        # æ‰¾å‡ºåº”è¯¥åˆå¹¶çš„è¯å¯¹
        for (w1, w2), count in bigram_counter.items():
            merged = w1 + w2
            if len(merged) > cfg.MERGE_MAX_LEN:
                continue
            if count < cfg.MERGE_MIN_FREQ:
                continue
            
            # æ¡ä»¶æ¦‚ç‡ P(w2|w1)
            if word_right_counter[w1] > 0:
                prob = count / word_right_counter[w1]
                if prob >= cfg.MERGE_MIN_PROB:
                    self.merged_words[merged] = (w1, w2, count, prob)
                    self.tokenizer.add_word(merged, freq=count * 1000)
        
        print(f"   åˆå¹¶ {len(self.merged_words)} ä¸ªè¯ç»„")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ª
        if self.merged_words:
            sorted_merges = sorted(self.merged_words.items(), key=lambda x: -x[1][2])[:10]
            for merged, (w1, w2, cnt, prob) in sorted_merges:
                print(f"      {merged}: {w1}+{w2} ({cnt}æ¬¡, {prob:.0%})")

    def _tokenize_and_count(self):
        """åˆ†è¯ç»Ÿè®¡"""
        for idx, msg in enumerate(self.messages):
            # è·³è¿‡æœºå™¨äººæ¶ˆæ¯
            if self._is_bot_message(msg):
                continue
            
            # è·³è¿‡è¢«è¿‡æ»¤çš„ç”¨æˆ·
            if self._should_filter_user(msg):
                continue
            
            sender_uin = msg.get('sender', {}).get('uin')
            content = msg.get('content', {})
            text = content.get('text', '') if isinstance(content, dict) else ''
            original_text = text
            cleaned = clean_text(text)
            
            if not cleaned:
                continue
            
            words = list(self.tokenizer.cut(cleaned))
            emojis = extract_emojis(cleaned)
            words = [w for w in words if not is_emoji(w)]  # æ–°å¢ï¼šä»wordsä¸­å»æ‰emoji
            all_tokens = words + emojis
            
            for word in all_tokens:
                word = word.strip()
                if not word:
                    continue
                
                # è¿‡æ»¤@ç¬¦å·åŠå…¶ç›¸å…³å†…å®¹ï¼ˆé¢å¤–æ£€æŸ¥ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼ï¼‰
                if word.startswith('@') or '@' in word:
                    continue
                
                # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœè¯æ±‡çœ‹èµ·æ¥åƒæ˜¯ç¾¤æ˜µç§°ï¼ˆå•ç‹¬å‡ºç°çš„è‹±æ–‡å•è¯ï¼Œä¸”å¯èƒ½æ˜¯è¿‡æ»¤@åæ®‹ç•™çš„ï¼‰
                # è¿™ç§æƒ…å†µåº”è¯¥å·²ç»åœ¨clean_textä¸­å¤„ç†ï¼Œä½†ä¸ºäº†ä¿é™©èµ·è§ï¼Œè¿™é‡Œä¹Ÿæ£€æŸ¥
                # æ³¨æ„ï¼šè¿™ä¸ªæ£€æŸ¥æ¯”è¾ƒä¿å®ˆï¼Œåªè¿‡æ»¤æ˜æ˜¾æ˜¯ç¾¤æ˜µç§°çš„æƒ…å†µ
                # å¦‚æœè¯æ±‡æ˜¯çº¯è‹±æ–‡å•è¯ä¸”é•¿åº¦è¾ƒçŸ­ï¼ˆå¯èƒ½æ˜¯ç¾¤æ˜µç§°ï¼‰ï¼Œä¸”ä¸åœ¨å¸¸ç”¨è¯åˆ—è¡¨ä¸­ï¼Œå¯èƒ½éœ€è¦è¿‡æ»¤
                # ä½†è¿™æ ·å¯èƒ½è¯¯åˆ ï¼Œæ‰€ä»¥æš‚æ—¶ä¸å¤„ç†ï¼Œè®©clean_textå‡½æ•°å¤„ç†
                
                # è·³è¿‡çº¯æ•°å­—/ç¬¦å·
                if re.match(r'^[\d\W]+$', word) and not is_emoji(word):
                    continue
                
                # è¿‡æ»¤åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å­—ç¬¦ä¸²ï¼ˆå¦‚7R%D8ã€åŒ…å«%ã€_ã€-ã€}ã€]ç­‰ï¼‰
                # è¿™äº›é€šå¸¸æ˜¯å›¾ç‰‡IDã€æ¶ˆæ¯IDç­‰æ— æ„ä¹‰æ ‡è¯†ç¬¦
                if re.search(r'[%_\-}\]]', word) and not re.search(r'[\u4e00-\u9fff]', word):
                    # å¦‚æœåŒ…å«ç‰¹æ®Šå­—ç¬¦ä¸”æ²¡æœ‰ä¸­æ–‡ï¼Œå¾ˆå¯èƒ½æ˜¯ID
                    continue
                
                # è¿‡æ»¤æ— æ„ä¹‰ç¬¦å·è¯æ±‡ï¼ˆå¦‚âŒ’ã€â˜†ã€â˜…ç­‰ï¼‰
                # å¦‚æœè¯åªåŒ…å«æ— æ„ä¹‰ç¬¦å·ï¼Œè·³è¿‡
                if all(c in MEANINGLESS_SYMBOLS for c in word):
                    continue
                # å¦‚æœè¯åŒ…å«æ— æ„ä¹‰ç¬¦å·ä¸”æ²¡æœ‰å…¶ä»–æœ‰æ„ä¹‰å­—ç¬¦ï¼Œè·³è¿‡
                if word and all(c in MEANINGLESS_SYMBOLS or c in string.punctuation or c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€""''ï¼ˆï¼‰ã€ã€‘' or c.isspace() for c in word):
                    continue
                
                # è¿‡æ»¤IDç±»å­—ç¬¦ä¸²ï¼ˆå›¾ç‰‡IDã€æ¶ˆæ¯IDç­‰ï¼‰
                # åŒ¹é…ï¼š3-20ä¸ªå­—ç¬¦ï¼Œä¸»è¦æ˜¯å­—æ¯æ•°å­—ç»„åˆï¼ŒåŒ…æ‹¬çŸ­IDå¦‚7R%D8ã€0ED3V
                if self._is_id_like_string(word):
                    continue
                
                # æå‰è¿‡æ»¤é»‘åå•ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šé¿å…ç»Ÿè®¡åå†è¿‡æ»¤ï¼‰
                if word in cfg.BLACKLIST:
                    continue
                
                # è¿‡æ»¤è™šè¯ï¼ˆä¸è®¡å…¥ç»Ÿè®¡ï¼‰
                if word in cfg.FUNCTION_WORDS:
                    continue
                
                # åŒè¯å¼‚æ ¼å¤„ç†ï¼šå°†åˆ«åæ˜ å°„åˆ°æ ‡å‡†è¯
                normalized_word = self._normalize_word(word)
                
                # ç»Ÿè®¡æ ‡å‡†è¯ï¼ˆå¦‚æœæ˜ å°„äº†ï¼Œç»Ÿè®¡æ ‡å‡†è¯ï¼›å¦åˆ™ç»Ÿè®¡åŸè¯ï¼‰
                self.word_freq[normalized_word] += 1
                if sender_uin:
                    self.word_contributors[normalized_word][sender_uin] += 1
                if len(self.word_samples[normalized_word]) < cfg.SAMPLE_COUNT * 3:
                    # åªæ”¶é›†æœ‰æ„ä¹‰çš„æ ·æœ¬ï¼ˆè¿‡æ»¤æ‰åªåŒ…å«å›¾ç‰‡æ ‡è®°ã€IDç­‰çš„æ— æ„ä¹‰å†…å®¹ï¼‰
                    if self._is_meaningful_sample(cleaned):
                        self.word_samples[normalized_word].append(cleaned)

    def _fun_statistics(self):
        """è¶£å‘³ç»Ÿè®¡"""
        prev_clean = None  # æ”¹ç”¨æ¸…ç†åæ–‡æœ¬
        prev_sender = None
        
        for msg in self.messages:
            # è·³è¿‡æœºå™¨äººæ¶ˆæ¯
            if self._is_bot_message(msg):
                continue
            
            # è·³è¿‡è¢«è¿‡æ»¤çš„ç”¨æˆ·
            if self._should_filter_user(msg):
                continue
            
            sender_uin = msg.get('sender', {}).get('uin')
            if not sender_uin:
                continue
            
            content = msg.get('content', {})
            text = content.get('text', '') if isinstance(content, dict) else ''
            timestamp = msg.get('timestamp', '')
            
            self.user_msg_count[sender_uin] += 1
            clean = clean_text(text)
            self.user_char_count[sender_uin] += len(clean)
            
            # å›¾ç‰‡æ£€æµ‹ï¼ˆæ’é™¤gifï¼‰
            if '[å›¾ç‰‡:' in text:
                if '.gif' not in text.lower():
                    self.user_image_count[sender_uin] += 1
            
            # è½¬å‘æ£€æµ‹
            if '[åˆå¹¶è½¬å‘:' in text:
                self.user_forward_count[sender_uin] += 1
            
            # å›å¤ç»Ÿè®¡
            reply_info = content.get('reply') if isinstance(content, dict) else None
            if reply_info:
                self.user_reply_count[sender_uin] += 1
                ref_msg_id = reply_info.get('referencedMessageId')
                if ref_msg_id and ref_msg_id in self.msgid_to_sender:
                    target_uin = self.msgid_to_sender[ref_msg_id]
                    self.user_replied_count[target_uin] += 1
            
            # @ç»Ÿè®¡
            raw = msg.get('rawMessage', {})
            elements = raw.get('elements', [])
            for elem in elements:
                if elem.get('elementType') == 1:
                    text_elem = elem.get('textElement', {})
                    at_type = text_elem.get('atType', 0)
                    at_uid = text_elem.get('atUid', '')
                    if at_type > 0 and at_uid and at_uid != '0':
                        self.user_at_count[sender_uin] += 1
                        self.user_ated_count[at_uid] += 1
                        # è®°å½•@çš„ç›®æ ‡ç”¨æˆ·
                        self.user_at_targets[sender_uin][at_uid] += 1
            
            # è¡¨æƒ…ç»Ÿè®¡ï¼ˆåŒ…æ‹¬emojiã€[è¡¨æƒ…:]ã€gifï¼‰
            emojis = extract_emojis(clean)
            gif_count = text.lower().count('.gif')
            bracket_emoji_count = text.count('[è¡¨æƒ…:')
            emoji_count = len(emojis) + bracket_emoji_count + gif_count
            if emoji_count > 0:
                self.user_emoji_count[sender_uin] += emoji_count
            
            # é“¾æ¥ç»Ÿè®¡
            if '[é“¾æ¥:' in text or re.search(r'https?://', text):
                self.user_link_count[sender_uin] += 1
            
            # æ—¶æ®µç»Ÿè®¡
            hour = parse_timestamp(timestamp)
            if hour is not None:
                self.hour_distribution[hour] += 1
                if hour in cfg.NIGHT_OWL_HOURS:
                    self.user_night_count[sender_uin] += 1
                if hour in cfg.EARLY_BIRD_HOURS:
                    self.user_morning_count[sender_uin] += 1
            
            # å¤è¯»ç»Ÿè®¡ï¼ˆç”¨æ¸…ç†åæ–‡æœ¬ï¼Œä¸”å†…å®¹è¦æœ‰æ„ä¹‰ï¼‰
            if clean and len(clean) >= 2:
                if clean == prev_clean and sender_uin != prev_sender:
                    self.user_repeat_count[sender_uin] += 1
            
            # æƒ…æ„Ÿåˆ†æç»Ÿè®¡
            if clean and len(clean) >= 2:
                sentiment = analyze_sentiment(clean)
                if sentiment == 'positive':
                    self.user_positive_count[sender_uin] += 1
                elif sentiment == 'negative':
                    self.user_negative_count[sender_uin] += 1
                else:
                    self.user_neutral_count[sender_uin] += 1
                
                # æ”¶é›†å‘è¨€æ ·æœ¬ï¼ˆæœ€å¤šä¿å­˜10æ¡æœ‰æ„ä¹‰çš„æ ·æœ¬ï¼‰
                if self._is_meaningful_sample(clean) and len(self.user_message_samples[sender_uin]) < 10:
                    # åªä¿å­˜é•¿åº¦é€‚ä¸­çš„æ ·æœ¬ï¼ˆ10-100å­—ç¬¦ï¼‰
                    if 10 <= len(clean) <= 100:
                        self.user_message_samples[sender_uin].append(clean)
            
            prev_clean = clean if clean else prev_clean  # ç©ºæ¶ˆæ¯ä¸æ›´æ–°
            prev_sender = sender_uin
        
        # è®¡ç®—äººå‡å­—æ•°
        for uin in self.user_msg_count:
            msg_count = self.user_msg_count[uin]
            char_count = self.user_char_count[uin]
            if msg_count >= 10:
                self.user_char_per_msg[uin] = char_count / msg_count

    def _filter_results(self):
        """è¿‡æ»¤ç»“æœ"""
        filtered_freq = Counter()
        
        for word, freq in self.word_freq.items():
            # é•¿åº¦è¿‡æ»¤
            if len(word) < cfg.MIN_WORD_LEN or len(word) > cfg.MAX_WORD_LEN:
                continue
            if freq < cfg.MIN_FREQ:
                continue
            
            # ç™½åå•ç›´æ¥é€šè¿‡
            if word in cfg.WHITELIST:
                filtered_freq[word] = freq
                continue
            
            # é»‘åå•è·³è¿‡
            if word in cfg.BLACKLIST:
                continue
            
            # è™šè¯è¿‡æ»¤ï¼ˆä¸è®¡å…¥ç»Ÿè®¡ï¼‰
            if word in cfg.FUNCTION_WORDS:
                continue
            
            # è¿‡æ»¤åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å­—ç¬¦ä¸²ï¼ˆå¦‚7R%D8ã€åŒ…å«%ã€_ã€-ã€}ã€]ç­‰ï¼‰
            # è¿™äº›é€šå¸¸æ˜¯å›¾ç‰‡IDã€æ¶ˆæ¯IDç­‰æ— æ„ä¹‰æ ‡è¯†ç¬¦
            if re.search(r'[%_\-}\]]', word) and not re.search(r'[\u4e00-\u9fff]', word):
                # å¦‚æœåŒ…å«ç‰¹æ®Šå­—ç¬¦ä¸”æ²¡æœ‰ä¸­æ–‡ï¼Œå¾ˆå¯èƒ½æ˜¯ID
                continue
            
            # è¿‡æ»¤IDç±»å­—ç¬¦ä¸²ï¼ˆå›¾ç‰‡IDã€æ¶ˆæ¯IDç­‰ï¼‰
            if self._is_id_like_string(word):
                continue
            
            # å•å­—ç‰¹æ®Šå¤„ç†ï¼ˆé‡‡ç”¨æ—§ç‰ˆé€»è¾‘ï¼‰
            if len(word) == 1:
                if is_emoji(word):
                    pass  # emojiä¿ç•™
                else:
                    stats = self.single_char_stats.get(word)
                    if stats:
                        total, indep, ratio = stats
                        if ratio < cfg.SINGLE_MIN_SOLO_RATIO or indep < cfg.SINGLE_MIN_SOLO_COUNT:
                            continue
                    else:
                        continue
            
            # çº¯æ•°å­—è·³è¿‡
            if re.match(r'^[\d\s]+$', word):
                continue
            
            # çº¯æ ‡ç‚¹è·³è¿‡
            if all(c in string.punctuation or c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€""''ï¼ˆï¼‰ã€ã€‘' for c in word):
                continue
            
            # è¿‡æ»¤æ— æ„ä¹‰ç¬¦å·è¯æ±‡ï¼ˆå¦‚âŒ’ã€â˜†ã€â˜…ç­‰ï¼‰
            # å¦‚æœè¯åªåŒ…å«æ— æ„ä¹‰ç¬¦å·ï¼Œè·³è¿‡
            if all(c in MEANINGLESS_SYMBOLS for c in word):
                continue
            # å¦‚æœè¯åŒ…å«æ— æ„ä¹‰ç¬¦å·ä¸”æ²¡æœ‰å…¶ä»–æœ‰æ„ä¹‰å­—ç¬¦ï¼Œè·³è¿‡
            if word and all(c in MEANINGLESS_SYMBOLS or c in string.punctuation or c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€""''ï¼ˆï¼‰ã€ã€‘' or c.isspace() for c in word):
                continue
            
            filtered_freq[word] = freq
        
        self.word_freq = filtered_freq
        
        # é‡‡æ ·å¹¶è¿‡æ»¤æ— æ„ä¹‰æ ·æœ¬
        for word in list(self.word_samples.keys()):
            samples = self.word_samples[word]
            # è¿‡æ»¤æ— æ„ä¹‰æ ·æœ¬
            meaningful_samples = [s for s in samples if self._is_meaningful_sample(s)]
            if len(meaningful_samples) > cfg.SAMPLE_COUNT:
                self.word_samples[word] = random.sample(meaningful_samples, cfg.SAMPLE_COUNT)
            else:
                self.word_samples[word] = meaningful_samples
        
        print(f"   è¿‡æ»¤å {len(self.word_freq)} ä¸ªè¯")

    def get_top_words(self, n=None):
        n = n or cfg.TOP_N
        return self.word_freq.most_common(n)

    def _is_filtered_user_by_uin(self, uin):
        """æ ¹æ®uinåˆ¤æ–­ç”¨æˆ·æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
        if not uin:
            return False
        # æ£€æŸ¥æ˜ å°„ä¸­çš„åç§°
        if uin in self.uin_to_name:
            name = self.uin_to_name[uin]
            for filtered_name in cfg.FILTERED_USERS:
                if filtered_name in name:
                    return True
        return False
    
    def _is_meaningful_sample(self, text):
        """åˆ¤æ–­æ ·æœ¬æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆè¿‡æ»¤æ‰åªåŒ…å«å›¾ç‰‡æ ‡è®°ã€IDç­‰çš„æ— æ„ä¹‰å†…å®¹ï¼‰"""
        if not text or len(text.strip()) < 2:
            return False
        
        # å»é™¤ç©ºç™½åæ£€æŸ¥
        text_clean = text.strip()
        
        # å†æ¬¡æ¸…ç†å›¾ç‰‡æ ‡è®°ï¼ˆç¡®ä¿å½»åº•æ¸…ç†ï¼‰
        text_clean = re.sub(r'\[å›¾ç‰‡[^\]]*\]', '', text_clean, flags=re.IGNORECASE)
        text_clean = re.sub(r'\[å›¾ç‰‡[^\[\]]*', '', text_clean, flags=re.IGNORECASE)
        text_clean = re.sub(r'[A-Z0-9]+`[A-Z0-9]+', '', text_clean)  # å»é™¤å›¾ç‰‡IDæ ¼å¼
        text_clean = text_clean.strip()
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè®¤ä¸ºæ— æ„ä¹‰
        if not text_clean:
            return False
        
        # å¦‚æœåªåŒ…å«å›¾ç‰‡æ ‡è®°ã€IDç­‰ï¼Œè®¤ä¸ºæ— æ„ä¹‰
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«ç±»ä¼¼å›¾ç‰‡IDçš„å­—ç¬¦ä¸²ï¼ˆå­—æ¯æ•°å­—+ç‰¹æ®Šå­—ç¬¦ï¼‰
        if re.match(r'^[A-Z0-9`\-_\s]+$', text_clean):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡æ ‡è®°æ®‹ç•™
        if '[å›¾ç‰‡' in text_clean.lower() or 'å›¾ç‰‡:' in text_clean.lower():
            return False
        
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«æ–¹æ‹¬å·å†…å®¹
        text_no_brackets = re.sub(r'\[[^\]]*\]', '', text_clean)
        if not text_no_brackets.strip():
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‡³å°‘ä¸€ä¸ªä¸­æ–‡å­—ç¬¦æˆ–å¸¸è§æ ‡ç‚¹
        if not re.search(r'[\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰]', text_clean):
            # å¦‚æœæ²¡æœ‰ä¸­æ–‡ï¼Œè‡³å°‘è¦æœ‰ä¸€äº›æœ‰æ„ä¹‰çš„è‹±æ–‡å•è¯ï¼ˆé•¿åº¦>=2ï¼‰
            words = re.findall(r'[a-zA-Z]{2,}', text_clean)
            if len(words) == 0:
                return False
        
        return True
    
    def get_word_detail(self, word):
        # è¿‡æ»¤æ‰è¢«è¿‡æ»¤ç”¨æˆ·çš„è´¡çŒ®è€…
        filtered_contributors = [
            (self.get_name(uin), count)
            for uin, count in self.word_contributors[word].most_common(cfg.CONTRIBUTOR_TOP_N * 2)
            if not self._is_filtered_user_by_uin(uin)
        ][:cfg.CONTRIBUTOR_TOP_N]  # å–å‰Nä¸ª
        
        return {
            'word': word,
            'freq': self.word_freq.get(word, 0),
            'samples': [s for s in self.word_samples.get(word, []) 
                       if self._is_meaningful_sample(s)],
            'contributors': filtered_contributors
        }

    def get_fun_rankings(self):
        rankings = {}
        
        def fmt(counter, top_n=cfg.RANK_TOP_N):
            return [(self.get_name(uin), count) for uin, count in counter.most_common(top_n)]
        
        rankings['è¯ç—¨æ¦œ'] = fmt(self.user_msg_count)
        rankings['å­—æ•°æ¦œ'] = fmt(self.user_char_count)
        
        sorted_avg = sorted(self.user_char_per_msg.items(), key=lambda x: x[1], reverse=True)[:cfg.RANK_TOP_N]
        rankings['é•¿æ–‡ç‹'] = [(self.get_name(uin), f"{avg:.1f}å­—/æ¡") for uin, avg in sorted_avg]
        
        rankings['å›¾ç‰‡ç‹‚é­”'] = fmt(self.user_image_count)
        rankings['åˆå¹¶è½¬å‘ç‹'] = fmt(self.user_forward_count)
        rankings['å›å¤ç‹‚'] = fmt(self.user_reply_count)
        rankings['è¢«å›å¤æœ€å¤š'] = fmt(self.user_replied_count)
        rankings['è‰¾ç‰¹ç‹‚'] = fmt(self.user_at_count)
        rankings['è¢«è‰¾ç‰¹æœ€å¤š'] = fmt(self.user_ated_count)
        rankings['è¡¨æƒ…å¸'] = fmt(self.user_emoji_count)
        rankings['é“¾æ¥åˆ†äº«ç‹'] = fmt(self.user_link_count)
        rankings['æ·±å¤œå…š'] = fmt(self.user_night_count)
        rankings['æ—©èµ·é¸Ÿ'] = fmt(self.user_morning_count)
        rankings['å¤è¯»æœº'] = fmt(self.user_repeat_count)
        
        return rankings
    
    def export_json(self):
        """å¯¼å‡ºJSONæ ¼å¼ç»“æœï¼ˆåŒ…å«uinä¿¡æ¯ï¼‰"""
        result = {
            'chatName': self.chat_name,
            'messageCount': len(self.messages),
            'topWords': [
                {
                    'word': word,
                    'freq': freq,
                    'contributors': [
                        {
                            'name': self.get_name(uin), 
                            'uin': uin,
                            'count': count
                        }
                        for uin, count in self.word_contributors[word].most_common(cfg.CONTRIBUTOR_TOP_N * 2)
                        if not self._is_filtered_user_by_uin(uin)
                    ][:cfg.CONTRIBUTOR_TOP_N],  # è¿‡æ»¤åå–å‰Nä¸ª
                    'samples': [s for s in self.word_samples.get(word, [])[:cfg.SAMPLE_COUNT * 2]
                               if self._is_meaningful_sample(s)][:cfg.SAMPLE_COUNT]
                }
                for word, freq in self.get_top_words()
            ],
            'rankings': {},
            'hourDistribution': {str(h): self.hour_distribution.get(h, 0) for h in range(24)}
        }
        
        # è¶£å‘³æ¦œå•ï¼ˆåŒ…å«uinï¼‰
        def fmt_with_uin(counter, top_n=cfg.RANK_TOP_N):
            return [
                {'name': self.get_name(uin), 'uin': uin, 'value': count}
                for uin, count in counter.most_common(top_n)
            ]
        
        result['rankings']['è¯ç—¨æ¦œ'] = fmt_with_uin(self.user_msg_count)
        result['rankings']['å­—æ•°æ¦œ'] = fmt_with_uin(self.user_char_count)
        
        # é•¿æ–‡ç‹ç‰¹æ®Šå¤„ç†
        sorted_avg = sorted(self.user_char_per_msg.items(), key=lambda x: x[1], reverse=True)[:cfg.RANK_TOP_N]
        result['rankings']['é•¿æ–‡ç‹'] = [
            {'name': self.get_name(uin), 'uin': uin, 'value': f"{avg:.1f}å­—/æ¡"}
            for uin, avg in sorted_avg
        ]
        
        result['rankings']['å›¾ç‰‡ç‹‚é­”'] = fmt_with_uin(self.user_image_count)
        result['rankings']['åˆå¹¶è½¬å‘ç‹'] = fmt_with_uin(self.user_forward_count)
        result['rankings']['å›å¤ç‹‚'] = fmt_with_uin(self.user_reply_count)
        result['rankings']['è¢«å›å¤æœ€å¤š'] = fmt_with_uin(self.user_replied_count)
        result['rankings']['è‰¾ç‰¹ç‹‚'] = fmt_with_uin(self.user_at_count)
        result['rankings']['è¢«è‰¾ç‰¹æœ€å¤š'] = fmt_with_uin(self.user_ated_count)
        result['rankings']['è¡¨æƒ…å¸'] = fmt_with_uin(self.user_emoji_count)
        result['rankings']['é“¾æ¥åˆ†äº«ç‹'] = fmt_with_uin(self.user_link_count)
        result['rankings']['æ·±å¤œå…š'] = fmt_with_uin(self.user_night_count)
        result['rankings']['æ—©èµ·é¸Ÿ'] = fmt_with_uin(self.user_morning_count)
        result['rankings']['å¤è¯»æœº'] = fmt_with_uin(self.user_repeat_count)
        
        return result
    
    def get_user_representative_words(self, top_n_users=10, words_per_user=5):
        """
        è·å–æ¯ä¸ªç”¨æˆ·çš„ä»£è¡¨æ€§è¯æ±‡
        
        Args:
            top_n_users: é€‰æ‹©å‰Nä¸ªæ´»è·ƒç”¨æˆ·
            words_per_user: æ¯ä¸ªç”¨æˆ·é€‰æ‹©Nä¸ªä»£è¡¨æ€§è¯æ±‡
            
        Returns:
            List[Dict]: æ¯ä¸ªç”¨æˆ·çš„ä¿¡æ¯ï¼ŒåŒ…å«name, uin, words(ä»£è¡¨æ€§è¯æ±‡åˆ—è¡¨), stats(ç»Ÿè®¡æ•°æ®)
        """
        # ä»word_contributorsåå‘ç»Ÿè®¡æ¯ä¸ªç”¨æˆ·ä½¿ç”¨çš„è¯æ±‡
        user_word_freq = defaultdict(Counter)  # {uin: {word: count}}
        
        for word, contributors in self.word_contributors.items():
            # è·³è¿‡æ— æ„ä¹‰è¯
            if word in cfg.FUNCTION_WORDS or word in cfg.BLACKLIST:
                continue
            # è·³è¿‡å•å­—ï¼ˆé™¤éæ˜¯emojiï¼‰
            if len(word) == 1 and not is_emoji(word):
                continue
            
            # è¿‡æ»¤å­—æ¯æ•°å­—ç»„åˆï¼ˆå¦‚5Cã€VXAç­‰ï¼‰
            if re.match(r'^[a-zA-Z0-9]+$', word) and len(word) <= 5:
                # å¦‚æœåªåŒ…å«å­—æ¯å’Œæ•°å­—ï¼Œä¸”é•¿åº¦è¾ƒçŸ­ï¼Œå¾ˆå¯èƒ½æ˜¯æ— æ„ä¹‰çš„IDæˆ–ä»£ç 
                # ä½†ä¿ç•™è¾ƒé•¿çš„æœ‰æ„ä¹‰ç»„åˆï¼ˆå¦‚"iPhone"ç­‰ï¼‰
                if not any(c.isalpha() and c.islower() for c in word):
                    # å¦‚æœå…¨æ˜¯å¤§å†™å­—æ¯å’Œæ•°å­—ï¼Œå¾ˆå¯èƒ½æ˜¯æ— æ„ä¹‰çš„
                    continue
            
            # è¿‡æ»¤ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚âŒ’ã€â˜†ç­‰ï¼‰
            if re.match(r'^[^\u4e00-\u9fff\w\s]+$', word):
                # åªåŒ…å«ç‰¹æ®Šç¬¦å·ï¼Œæ²¡æœ‰ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
                continue
            
            # è¿‡æ»¤çº¯ç¬¦å·ç»„åˆï¼ˆä½¿ç”¨ç»Ÿä¸€çš„MEANINGLESS_SYMBOLSï¼‰
            if all(c in MEANINGLESS_SYMBOLS for c in word):
                continue
            
            for uin, count in contributors.items():
                # è·³è¿‡è¢«è¿‡æ»¤çš„ç”¨æˆ·
                if self._is_filtered_user_by_uin(uin):
                    continue
                user_word_freq[uin][word] += count
        
        # é€‰æ‹©æœ€æ´»è·ƒçš„top_n_usersä¸ªç”¨æˆ·ï¼ˆæŒ‰æ¶ˆæ¯æ•°ï¼‰
        top_users = [uin for uin, _ in self.user_msg_count.most_common(top_n_users * 2)]
        # è¿‡æ»¤æ‰è¢«è¿‡æ»¤çš„ç”¨æˆ·
        top_users = [uin for uin in top_users if not self._is_filtered_user_by_uin(uin)][:top_n_users]
        
        result = []
        for uin in top_users:
            user_words = user_word_freq.get(uin, Counter())
            if not user_words:
                continue
            
            # é€‰æ‹©æ¯ä¸ªç”¨æˆ·æœ€æœ‰ä»£è¡¨æ€§çš„words_per_userä¸ªè¯
            # ä¼˜å…ˆé€‰æ‹©ï¼š1. é¢‘ç‡é«˜ 2. ä¸æ˜¯æ— æ„ä¹‰è¯ 3. æœ‰å®é™…æ„ä¹‰
            selected_words = []
            for word, count in user_words.most_common(words_per_user * 5):
                # å†æ¬¡è¿‡æ»¤æ— æ„ä¹‰è¯
                if word in cfg.FUNCTION_WORDS or word in cfg.BLACKLIST:
                    continue
                # è·³è¿‡å•å­—ï¼ˆé™¤éæ˜¯emojiï¼‰
                if len(word) == 1 and not is_emoji(word):
                    continue
                # è·³è¿‡çº¯æ•°å­—/ç¬¦å·
                if re.match(r'^[\d\W]+$', word) and not is_emoji(word):
                    continue
                
                # è¿‡æ»¤å­—æ¯æ•°å­—ç»„åˆï¼ˆå¦‚5Cã€VXAç­‰ï¼‰
                if re.match(r'^[a-zA-Z0-9]+$', word) and len(word) <= 5:
                    # å¦‚æœåªåŒ…å«å­—æ¯å’Œæ•°å­—ï¼Œä¸”é•¿åº¦è¾ƒçŸ­ï¼Œå¾ˆå¯èƒ½æ˜¯æ— æ„ä¹‰çš„IDæˆ–ä»£ç 
                    # ä½†ä¿ç•™è¾ƒé•¿çš„æœ‰æ„ä¹‰ç»„åˆï¼ˆå¦‚"iPhone"ç­‰ï¼‰
                    if not any(c.isalpha() and c.islower() for c in word):
                        # å¦‚æœå…¨æ˜¯å¤§å†™å­—æ¯å’Œæ•°å­—ï¼Œå¾ˆå¯èƒ½æ˜¯æ— æ„ä¹‰çš„
                        continue
                
                # è¿‡æ»¤ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚âŒ’ã€â˜†ç­‰ï¼‰
                if re.match(r'^[^\u4e00-\u9fff\w\s]+$', word):
                    # åªåŒ…å«ç‰¹æ®Šç¬¦å·ï¼Œæ²¡æœ‰ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
                    continue
                
                # è¿‡æ»¤çº¯ç¬¦å·ç»„åˆï¼ˆä½¿ç”¨ç»Ÿä¸€çš„MEANINGLESS_SYMBOLSï¼‰
                if all(c in MEANINGLESS_SYMBOLS for c in word):
                    continue
                
                selected_words.append({
                    'word': word,
                    'count': count
                })
                if len(selected_words) >= words_per_user:
                    break
            
            if not selected_words:
                continue
            
            # è·å–ç”¨æˆ·ç»Ÿè®¡æ•°æ®
            message_count = self.user_msg_count.get(uin, 0)
            char_count = self.user_char_count.get(uin, 0)
            emoji_count = self.user_emoji_count.get(uin, 0)
            
            # è®¡ç®—å¹³å‡æ¯å°æ—¶å‘è¨€æ•°ï¼ˆå‡è®¾åˆ†æçš„æ—¶é—´è·¨åº¦ï¼Œè¿™é‡Œç”¨æ€»æ¶ˆæ¯æ•°ä¼°ç®—ï¼‰
            # å¦‚æœæ— æ³•å‡†ç¡®è®¡ç®—ï¼Œä½¿ç”¨æ€»æ¶ˆæ¯æ•°ä½œä¸ºå‚è€ƒ
            total_messages = len(self.messages)
            if total_messages > 0:
                # ä¼°ç®—ï¼šå‡è®¾ç¾¤èŠæ´»è·ƒæœŸä¸º30å¤©ï¼Œæ¯å¤©24å°æ—¶
                estimated_hours = 30 * 24
                messages_per_hour = message_count / estimated_hours if estimated_hours > 0 else 0
            else:
                messages_per_hour = 0
            
            # æƒ…æ„Ÿç»Ÿè®¡
            positive_count = self.user_positive_count.get(uin, 0)
            negative_count = self.user_negative_count.get(uin, 0)
            neutral_count = self.user_neutral_count.get(uin, 0)
            total_sentiment = positive_count + negative_count + neutral_count
            if total_sentiment > 0:
                positive_ratio = positive_count / total_sentiment
                negative_ratio = negative_count / total_sentiment
                neutral_ratio = neutral_count / total_sentiment
            else:
                positive_ratio = negative_ratio = neutral_ratio = 0
            
            # æœ€å¸¸@çš„ç¾¤å‹ï¼ˆå‰3åï¼‰
            at_targets = self.user_at_targets.get(uin, Counter())
            top_at_targets = []
            for target_uin, count in at_targets.most_common(3):
                target_name = self.get_name(target_uin)
                top_at_targets.append({'name': target_name, 'count': count})
            
            # æœ€å¸¸ç”¨çš„è¡¨æƒ…ï¼ˆä»æ ·æœ¬ä¸­æå–ï¼‰
            user_samples = self.user_message_samples.get(uin, [])
            emoji_list = []
            for sample in user_samples[:20]:  # åªåˆ†æå‰20ä¸ªæ ·æœ¬
                emojis = extract_emojis(sample)
                emoji_list.extend(emojis)
            top_emojis = [emoji for emoji, _ in Counter(emoji_list).most_common(3)]
            
            user_stats = {
                'message_count': message_count,
                'char_count': char_count,
                'avg_chars_per_msg': self.user_char_per_msg.get(uin, 0),
                'messages_per_hour': round(messages_per_hour, 2),
                'emoji_count': emoji_count,
                'emoji_usage_rate': round(emoji_count / message_count, 2) if message_count > 0 else 0,
                'top_emojis': top_emojis,
                'sentiment': {
                    'positive_count': positive_count,
                    'negative_count': negative_count,
                    'neutral_count': neutral_count,
                    'positive_ratio': round(positive_ratio, 2),
                    'negative_ratio': round(negative_ratio, 2),
                    'neutral_ratio': round(neutral_ratio, 2),
                },
                'top_at_targets': top_at_targets,
                'message_samples': user_samples[:5]  # æœ€å¤š5ä¸ªæ ·æœ¬ç”¨äºAIä¸¾ä¾‹
            }
            
            result.append({
                'name': self.get_name(uin),
                'uin': uin,
                'words': selected_words,
                'stats': user_stats
            })
        
        return result
    
    def _is_filtered_user_by_uin(self, uin):
        """æ ¹æ®uinåˆ¤æ–­ç”¨æˆ·æ˜¯å¦è¢«è¿‡æ»¤"""
        if not uin:
            return True
        
        name = self.uin_to_name.get(uin, '')
        if not name:
            return False
        
        for filtered_name in cfg.FILTERED_USERS:
            if filtered_name in name:
                return True
        
        return False
