# -*- coding: utf-8 -*-
"""
QQ群年度报告分析器 - 配置文件

本文件包含命令行工具的所有可调参数，复制为 config.py 后修改使用
注意：后端服务配置请在 backend/.env 中设置
"""

# ============================================
# 基础配置
# ============================================

# 输入文件路径
# 使用 qq-chat-exporter 导出的 JSON 文件
# 示例：~/.qq-chat-exporter/exports/group_123456_20241212.json
INPUT_FILE = "chat.json"

# 输出编码
OUTPUT_ENCODING = "utf-8"

# 控制台输出宽度
CONSOLE_WIDTH = 60

# ============================================
# 分词器配置
# ============================================

# 分词器类型
# 'jieba' - 使用jieba分词（默认，速度快）
# 'pkuseg' - 使用pkuseg分词（准确率高，推荐用于对准确率要求高的场景）
# 'subword' - 使用SentencePiece subword分词（需要安装sentencepiece和训练模型）
TOKENIZER_TYPE = 'pkuseg'

# jieba分词配置
# 是否使用HMM模型识别未登录词（推荐开启，提高准确率）
JIEBA_USE_HMM = True

# 是否使用paddle模式（需要安装paddlepaddle，准确率更高但速度较慢）
# 安装命令: pip install paddlepaddle
JIEBA_USE_PADDLE = False

# pkuseg领域模型（仅pkuseg模式）
# 可选: 'news'(新闻), 'web'(网络), 'medicine'(医学), 'tourism'(旅游)
# 如果为None，使用默认模型
PKUSEG_MODEL = 'web'  # 例如: 'web' 或 'news'

# SentencePiece模型路径（仅subword模式需要）
# 如果为空，将尝试从数据自动训练模型
SP_MODEL_PATH = None

# SentencePiece词汇表大小（仅训练新模型时使用）
SP_VOCAB_SIZE = 8000

# SentencePiece模型类型
# 'bpe' - Byte Pair Encoding（推荐）
# 'unigram' - Unigram Language Model
SP_MODEL_TYPE = 'bpe'

# 自定义词典文件路径（仅jieba模式有效）
# 支持多个文件，用列表形式，例如：['dict1.txt', 'dict2.txt']
# 词典文件格式：每行一个词，格式为：词语 词频 词性（可选）
# 例如：
#   自定义词 1000
#   网络新词 2000 n
#   群友昵称 3000
# 如果词频和词性不指定，默认词频为1000
CUSTOM_DICT_FILES = []
# 示例：
# CUSTOM_DICT_FILES = ['custom_dict.txt', 'user_names.txt']


# ============================================
# 词频统计参数
# ============================================

# 提取前 N 个高频词
# 推荐值：100-300，过大会包含太多低频词
TOP_N = 200

# 最小词频阈值
# 低于此频率的词会被过滤，推荐值：1-5
MIN_FREQ = 1

# 词长度限制
MIN_WORD_LEN = 1    # 最小词长（字符数）
MAX_WORD_LEN = 10   # 最大词长（字符数）


# ============================================
# 新词发现参数
# ============================================

# PMI（点互信息）阈值
# 衡量词组内部的紧密程度，越高表示越像一个词
# 推荐值：1.5-3.0，调高会减少新词但提高质量
PMI_THRESHOLD = 2.0

# 信息熵阈值
# 衡量词组边界的确定性，越高表示边界越清晰
# 推荐值：0.3-1.0，调高会减少新词但提高质量
ENTROPY_THRESHOLD = 0.5

# 新词最小频次
# 出现次数少于此值的词组不会被识别为新词
# 推荐值：10-30
NEW_WORD_MIN_FREQ = 20


# ============================================
# 词组合并参数
# ============================================

# 词组合并最小频次
# 相邻词组出现次数少于此值不会被合并
# 推荐值：20-50
MERGE_MIN_FREQ = 30

# 词组合并条件概率阈值
# P(w2|w1) 大于此值才会合并，表示 w1 后跟 w2 的概率
# 推荐值：0.2-0.5
MERGE_MIN_PROB = 0.3

# 词组合并最大长度
# 合并后的词组最大长度（字符数）
MERGE_MAX_LEN = 6


# ============================================
# 单字过滤参数
# ============================================

# 单字独立出现比例阈值
# 单字作为独立词出现的比例（独立出现次数/总出现次数）
# 低于此值的单字会被过滤（emoji 除外）
# 推荐值：0.01-0.05
SINGLE_MIN_SOLO_RATIO = 0.01

# 单字独立出现次数阈值
# 单字作为独立词出现的最少次数
# 低于此值的单字会被过滤（emoji 除外）
# 推荐值：3-10
SINGLE_MIN_SOLO_COUNT = 5


# ============================================
# 词汇配置
# ============================================

# 白名单：这些词会被强制保留，不受任何过滤规则影响
WHITELIST = set([
    # 示例：
    # '群友昵称',
    # '特殊词汇',
])

# 虚词列表：这些无意义的虚词在统计时会被过滤掉，不计入词频统计
# 包含常见的语气词、助词、连词等无实际意义的词
FUNCTION_WORDS = set([
    # 特殊标记词
    '图片',
    # 语气词
    '啊', '呀', '呢', '吧', '吗', '嘛', '啦', '哦', '噢', '喔', '诶', '唉', '嗯', '哼', '哈',
    # 助词
    '的', '地', '得', '了', '着', '过', '所', '被', '把', '给', '向', '对', '为', '与', '和',
    # 连词
    '但', '但是', '而', '且', '或', '及', '以及', '或者', '还是', '然后', '接着', '所以', '因为', '由于',
    # 副词（无实际意义）
    '很', '非常', '特别', '尤其', '更', '最', '太', '也', '还', '又', '再', '就', '才', '都', '全', '只', '仅',
    # 代词（在统计中通常无意义）
    '我', '你', '他', '她', '它', '我们', '你们', '他们', '她们', '它们', '这', '那', '这个', '那个', '这些', '那些',
    # 数词（单独出现时）
    '一', '二', '三', '四', '五', '六', '七', '八', '九', '十', '个', '一个', '两个', '几个',
    # 其他常见无意义词
    '没', '没有', '不是', '什么', '怎么', '为什么', '哪里', '哪个', '多少', '在', '有', '是', '好', '会', '现在',
    '上', '下', '中', '里', '内', '外', '前', '后', '左', '右', '东', '南', '西', '北',
    # 高频无意义词（群友分析中需要过滤）
    '可以', '应该', '可能', '能够', '需要', '想要', '觉得', '感觉', '知道', '了解', '明白', '清楚',
    '这个', '那个', '这样', '那样', '这里', '那里', '这样', '那样',
    # 26个英文字母（单个字母通常无意义）
    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',
])

# 黑名单：这些词会被强制过滤掉（包含敏感词+其他需要过滤的词）
BLACKLIST = set([
    # 无意义的字母数字组合（如5C、VXA等）
    # 特殊符号（如⌒、☆等会在代码中通过正则过滤）
    # 示例：
    # '广告词',
    # '敏感词',
])

# ============================================
# 群聊名称处理配置
# ============================================

# 群聊名称到词汇的映射
# 当群聊名称匹配时，自动添加指定的词汇到词典
# 格式：{群聊名称或关键词: [要添加的词汇列表]}
# 注意：匹配是包含关系，如果群名包含关键词就会添加对应词汇
CHAT_NAME_WORDS = {
    # 示例：如果群名包含"重塑之手"、"重返未来"或"1999"，则添加"1999"和"深蓝"
    '重塑之手': ['1999', '深蓝'],
    '重返未来': ['1999', '深蓝'],
    '1999': ['1999', '深蓝'],
}

# ============================================
# 同词异格处理配置（同义词映射）
# ============================================

# 同词异格映射：将某些词映射到标准词
# 格式：{别名/变体: 标准词}
# 说明：在统计词频时，会将别名统一映射到标准词，这样"浅红"和"深蓝"会被统计为同一个词
# 例如："浅红"可能在说"深蓝"，"掰掰牙齿"可能指"牙仙"
WORD_ALIAS_MAP = {
    # 示例（根据实际需求修改）：
    '浅红': '深蓝',
    '掰掰牙齿': '牙仙',
    '牙线': '牙仙',
    '拜拜牙齿': '牙仙',
}


# ============================================
# 排行榜配置
# ============================================

# 各类排行榜显示的前 N 名
RANK_TOP_N = 10

# 热词贡献者显示的前 N 名
CONTRIBUTOR_TOP_N = 10

# 每个热词显示的示例消息数量
SAMPLE_COUNT = 10


# ============================================
# 时间分析配置
# ============================================

# 夜猫子时段（0-6点）
NIGHT_OWL_HOURS = range(0, 6)

# 早起鸟时段（6-9点）
EARLY_BIRD_HOURS = range(6, 9)


# ============================================
# 机器人过滤
# ============================================

# 是否过滤 QQ 机器人消息
# True：过滤掉机器人消息（推荐）
# False：保留所有消息
FILTER_BOT_MESSAGES = True

# 需要过滤的用户名列表（这些用户的消息不会被统计）
# 支持部分匹配，例如 "Q群管家" 会匹配 "Q群管家"、"Q群管家机器人" 等
FILTERED_USERS = [
    'Q群管家',
    '群管家',
    '饥荒服务器助手',
    '未知用户',
]


# ============================================
# AI 功能配置（可选）
# ============================================

# OpenAI 配置优先级说明：
# 1. 推荐方式：在 backend/.env 中配置（更安全，不会被 git 追踪）
# 2. 备用方式：在此文件中配置（不推荐，容易泄露密钥）
# 
# 如果两处都配置，backend/.env 的优先级更高
# main.py 会自动加载 backend/.env 中的环境变量

# OpenAI API Key
# 留空则不使用 AI 功能
# 获取地址：https://platform.openai.com/api-keys
# ⚠️ 建议在 backend/.env 中配置 OPENAI_API_KEY
OPENAI_API_KEY = ""

# OpenAI API 基础地址
# 官方地址：https://api.openai.com/v1
# 也可使用代理地址或其他兼容 OpenAI API 的服务
# ⚠️ 建议在 backend/.env 中配置 OPENAI_BASE_URL
OPENAI_BASE_URL = ""

# 使用的模型
# 推荐：gpt-4, gpt-4-turbo, gpt-3.5-turbo
# ⚠️ 建议在 backend/.env 中配置 OPENAI_MODEL
OPENAI_MODEL = ""

# AI 点评模式
# 'always'  - 总是生成 AI 点评
# 'never'   - 从不生成 AI 点评
# 'ask'     - 每次询问用户（默认）
AI_COMMENT_MODE = 'ask'


# ============================================
# 图片导出配置
# ============================================

# 是否启用图片导出功能
# True：启用图片导出（需要安装 Playwright）
# False：仅生成 HTML 报告
ENABLE_IMAGE_EXPORT = True

# 图片生成模式
# 'always'  - 总是生成图片
# 'never'   - 从不生成图片
# 'ask'     - 每次询问用户（默认）
IMAGE_GENERATION_MODE = 'ask'


# ============================================
# 高级配置（一般不需要修改）
# ============================================

# 如需添加自定义配置，请在下方添加
